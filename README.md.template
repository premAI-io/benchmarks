# benchmarks
MLOps Engines, Frameworks, and Languages benchmarks over main stream AI Models.

## Structure

The repository is organized to facilitate benchmark management and execution through a consistent structure:

- Each benchmark, identified as `bench_name`, has a dedicated folder, `bench_{bench_name}`.
- Within these benchmark folders, a common script named `bench.sh` handles setup, environment configuration, and execution.

### Benchmark Script

The `bench.sh` script supports key parameters:

- `prompt`: Benchmark-specific prompt.
- `max_tokens`: Maximum tokens for the benchmark.
- `repetitions`: Number of benchmark repetitions.
- `log_file`: File for storing benchmark logs.
- `device`: Device for benchmark execution (cpu, cuda, metal).
- `models_dir`: Directory containing necessary model files.

### Unified Execution

An overarching `bench.sh` script streamlines benchmark execution:

- Downloads essential files for benchmarking.
- Iterates through all benchmark folders in the repository.

This empowers users to seamlessly execute benchmarks based on their preference. To run a specific benchmark, navigate to the corresponding benchmark folder (e.g., `bench_{bench_name}`) and execute the `bench.sh` script with the required parameters.



## Usage

```bash
# Run a specific benchmark
./bench_{bench_name}/bench.sh --prompt <value> --max_tokens <value> --num_repetitions <value> --log_file <file_path> --device <cpu/cuda/metal> --models_dir <path_to_models>

# Run all benchmarks collectively
./bench.sh --prompt <value> --max_tokens <value> --num_repetitions <value> --log_file <file_path> --device <cpu/cuda/metal> --models_dir <path_to_models>
```


## ML Engines: Feature Table

| Features                    | pytorch | burn | llama.cpp | candle | tinygrad | onnxruntime | CTranslate2 |
| --------------------------- | ------- | ---- | --------- | ------ | -------- | ----------- | ----------- |
| Inference support           | âœ…      | âœ…   | âœ…        | âœ…     | âœ…       | âœ…          | âœ…          |
| 16-bit quantization support | âœ…      | âœ…   | âœ…        | âœ…     | âœ…       | âœ…          | âœ…          |
| 8-bit quantization support  | âœ…      | âŒ   | âœ…        | âœ…     | âœ…       | âœ…          | âœ…          |
| 4-bit quantization support  | âœ…      | âŒ   | âœ…        | âœ…     | âŒ       | âŒ          | âŒ          |
| 2/3bit quantization support | âœ…      | âŒ   | âœ…        | âœ…     | âŒ       | âŒ          | âŒ          |
| CUDA support                | âœ…      | âœ…   | âœ…        | âœ…     | âœ…       | âœ…          | âœ…          |
| ROCM support                | âœ…      | âœ…   | âœ…        | âœ…     | âœ…       | âŒ          | âŒ          |
| Intel OneAPI/SYCL support   | âœ…**    | âœ…   | âœ…        | âœ…     | âœ…       | âŒ          | âŒ          |
| Mac M1/M2 support           | âœ…      | âœ…   | âœ…        | â­     | âœ…       | âœ…          | â­          |
| BLAS support(CPU)           | âœ…      | âœ…   | âœ…        | âœ…     | âŒ       | âœ…          | âœ…          |
| Model Parallel support      | âœ…      | âŒ   | âŒ        | âœ…     | âŒ       | âŒ          | âœ…          |
| Tensor Parallel support     | âœ…      | âŒ   | âŒ        | âœ…     | âŒ       | âŒ          | âœ…          |
| Onnx Format support         | âœ…      | âœ…   | âœ…        | âœ…     | âœ…       | âœ…          | âŒ          |
| Training support            | âœ…      | ğŸŒŸ   | âŒ        | ğŸŒŸ     | âŒ       | âŒ          | âŒ          |

â­ = No Metal Support
ğŸŒŸ = Partial Support for Training (Finetuning already works, but training from scratch may not work)

## Benchmarking ML Engines

### A100 80GB Inference Bench:

Model: LLAMA-2-7B

CUDA Version: 11.7

Command: `./benchmark.sh --repetitions 10 --max_tokens 100 --device cuda --prompt 'Explain what is a transformer'`

| Engine      | float32      | float16       | int8          | int4          |
|-------------|--------------|---------------|---------------|---------------|
| burn        | 13.12 Â± 0.85 |      -        |      -        |      -        |
| candle      |      -       | 36.78 Â± 2.17  |      -        |      -        |
| llama.cpp   |      -       |      -        | 84.48 Â± 3.76  | 106.76 Â± 1.29 |
| ctranslate  |      -       | 51.38 Â± 16.01 | 36.12 Â± 11.93 |      -        |
| tinygrad    |      -       | 20.32 Â± 0.06  |      -        |      -        |
| onnx        |      -       | 54.16 Â± 3.15  |      -        |      -        |

*(data updated: <LAST_UPDATE>)


### M2 MAX 32GB Inference Bench:

#### CPU

Model: LLAMA-2-7B

CUDA Version: NA

Command: `./benchmark.sh --repetitions 10 --max_tokens 100 --device cpu --prompt 'Explain what is a transformer'`

| Engine      | float32      | float16      | int8         | int4         |
|-------------|--------------|--------------|--------------|--------------|
| burn        | 0.30 Â± 0.09  |      -       |      -       |      -       |
| candle      |      -       | 3.43 Â± 0.02  |      -       |      -       |
| llama.cpp   |      -       |      -       | 14.41 Â± 1.59 | 20.96 Â± 1.94 |
| ctranslate  |      -       |      -       | 2.11 Â± 0.73  |      -       |
| tinygrad    |      -       | 4.21 Â± 0.38  |      -       |      -       |
| onnx        |      -       |      -       |      -       |      -       |

#### GPU (Metal)

Command: `./benchmark.sh --repetitions 10 --max_tokens 100 --device metal --prompt 'Explain what is a transformer'`

| Engine      | float32      | float16      | int8         | int4         |
|-------------|--------------|--------------|--------------|--------------|
| burn        |      -       |      -       |      -       |      -       |
| candle      |      -       |      -       |      -       |      -       |
| llama.cpp   |      -       |      -       | 31.24 Â± 7.82 | 46.75 Â± 9.55 |
| ctranslate  |      -       |      -       |      -       |      -       |
| tinygrad    |      -       | 29.78 Â± 1.18 |      -       |      -       |
| onnx        |      -       |      -       |      -       |      -       |

*(data updated: <LAST_UPDATE>)
