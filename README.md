# benchmarks
MLOps Engines, Frameworks, and Languages benchmarks over main stream AI Models.

## Tool

The benchmarking tool comprises three main scripts:
- `benchmark.sh` for running the end-to-end benchmarking
- `download.sh` which is internally used by the benchmark script to download the needed model files based on a configuration
- `setup.sh` script for setup of dependencies and needed formats conversion

### benchmark

This script runs benchmarks for a transformer model using both Rust and Python implementations. It provides options to customize the benchmarks, such as the prompt, repetitions, maximum tokens, device, and NVIDIA flag.

```bash
./benchmark.sh [OPTIONS]
```
where `OPTIONS`:
- `-p, --prompt`: Prompt for benchmarks (default: 'Explain what is a transformer')
- `-r, --repetitions`: Number of repetitions for benchmarks (default: 2)
- `-m, --max_tokens`: Maximum number of tokens for benchmarks (default: 100)
- `-d, --device`: Device for benchmarks (possible values: 'gpu' or 'cpu', default: 'cpu')
- `--nvidia`: Use NVIDIA for benchmarks (default: false)

### download

Downloads files from a list of URLs specified in a JSON file. The JSON file should contain an array of objects, each with a 'url', 'file', and 'folder' property. The script checks if the file already exists before downloading it.

```bash
./download.sh --models <json_file> --cache <cache_file> --force-download
```
Options
- `--models`: JSON file specifying the models to download (default: models.json)
- `--cache`: Cache file to keep track of downloaded files (default: cache.log)
- `--force-download`: Force download of all files, removing existing files and cache

### setup
1. Creates a python virtual environment `venv` and installs project requirements.
3. Converts and stores models in different formats.

```bash
./setup.sh
```

## ML Engines: Feature Table

| Features                    | pytorch | burn | llama.cpp | candle | tinygrad | onnxruntime | CTranslate2 |
| --------------------------- | ------- | ---- | --------- | ------ | -------- | ----------- | ----------- |
| Inference support           | âœ…      | âœ…   | âœ…        | âœ…     | âœ…       | âœ…          | âœ…          |
| 16-bit quantization support | âœ…      | âœ…   | âœ…        | âœ…     | âœ…       | âœ…          | âœ…          |
| 8-bit quantization support  | âœ…      | âŒ   | âœ…        | âœ…     | âœ…       | âœ…          | âœ…          |
| 4-bit quantization support  | âœ…      | âŒ   | âœ…        | âœ…     | âŒ       | âŒ          | âŒ          |
| 2/3bit quantization support | âœ…      | âŒ   | âœ…        | âœ…     | âŒ       | âŒ          | âŒ          |
| CUDA support                | âœ…      | âœ…   | âœ…        | âœ…     | âœ…       | âœ…          | âœ…          |
| ROCM support                | âœ…      | âœ…   | âœ…        | âœ…     | âœ…       | âŒ          | âŒ          |
| Intel OneAPI/SYCL support   | âœ…**    | âœ…   | âœ…        | âœ…     | âœ…       | âŒ          | âŒ          |
| Mac M1/M2 support           | âœ…      | âœ…   | âœ…        | â­     | âœ…       | âœ…          | â­          |
| BLAS support(CPU)           | âœ…      | âœ…   | âœ…        | âœ…     | âŒ       | âœ…          | âœ…          |
| Model Parallel support      | âœ…      | âŒ   | âŒ        | âœ…     | âŒ       | âŒ          | âœ…          |
| Tensor Parallel support     | âœ…      | âŒ   | âŒ        | âœ…     | âŒ       | âŒ          | âœ…          |
| Onnx Format support         | âœ…      | âœ…   | âœ…        | âœ…     | âœ…       | âœ…          | âŒ          |
| Training support            | âœ…      | ğŸŒŸ   | âŒ        | ğŸŒŸ     | âŒ       | âŒ          | âŒ          |

â­ = No Metal Support
ğŸŒŸ = Partial Support for Training (Finetuning already works, but training from scratch may not work)

## Benchmarking ML Engines

### A100 80GB Inference Bench:

Model: LLAMA-2-7B

CUDA Version: 11.7

Command: `./benchmark.sh --repetitions 10 --max_tokens 100 --device gpu --nvidia --prompt 'Explain what is a transformer'`

| Engine      | float32      | float16      | int8         | int4         |
|-------------|--------------|--------------|--------------|--------------|
| burn        | 13.28 Â± 0.79 |      -       |      -       |      -       |
| candle      |      -       | 26.30 Â± 0.29 |      -       |      -       |
| llama.cpp   |      -       |      -       | 67.64 Â± 22.57| 106.21 Â± 2.21|
| ctranslate  |      -       | 58.54 Â± 13.24| 34.22 Â± 6.29 |      -       |
| tinygrad    |      -       | 20.13 Â± 1.35 |      -       |      -       |

*(data updated: 15th November 2023)


### M2 MAX 32GB Inference Bench:

#### CPU

Model: LLAMA-2-7B

CUDA Version: NA

Command: `./benchmark.sh --repetitions 10 --max_tokens 100 --device cpu --prompt 'Explain what is a transformer'`

| Engine      | float32       | float16       | int8         | int4         |
|-------------|--------------|--------------|--------------|--------------|
| burn        | 0.30 Â± 0.09  |      -       |      -       |      -       |
| candle      |      -       | 3.43 Â± 0.02  |      -       |      -       |
| llama.cpp   |      -       |      -       | 14.41 Â± 1.59 | 20.96 Â± 1.94 |
| ctranslate  |      -       |      -       | 2.11 Â± 0.73  |      -       |
| tinygrad    |      -       | 4.21 Â± 0.38  |      -       |      -       |

#### GPU (Metal)

Command: `./benchmark.sh --repetitions 10 --max_tokens 100 --device gpu --prompt 'Explain what is a transformer'`

| Engine      | float32       | float16       | int8         | int4         |
|-------------|--------------|--------------|--------------|--------------|
| burn        |      -       |      -       |      -       |      -       |
| candle      |      -       |      -       |      -       |      -       |
| llama.cpp   |      -       |      -       | 31.24 Â± 7.82 | 46.75 Â± 9.55 |
| ctranslate  |      -       |      -       |      -       |      -       |
| tinygrad    |      -       | 29.78 Â± 1.18 |      -       |      -       |

*(data updated: 15th November 2023)