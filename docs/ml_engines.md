# ğŸ”§ ML Engines

### Model Framework Support Matrix

| Engine                                     | Float32 | Float16 | Float8 | Int8  | Int4  | CUDA  | ROCM  | Mac M1/M2 | Training |
| ------------------------------------------ | :-----: | :-----: | :----: | :---: | :---: | :---: | :---: | :-------: | :------: |
| [candle](/bench_candle/)                   |    âš ï¸    |    âœ…    |   âŒ    |   âš ï¸   |   âš ï¸   |   âœ…   |   âŒ   |     âœ…     |    âŒ     |
| [llama.cpp](/bench_llamacpp/)              |    âŒ    |    âŒ    |   âŒ    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âŒ     |
| [ctranslate](/bench_ctranslate/)           |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |
| [onnx](/bench_onnxruntime/)                |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |
| [transformers (pytorch)](/bench_pytorch/)  |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |
| [vllm](/bench_vllm/)                       |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |
| [exllamav2](/bench_exllamav2/)             |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |
| [ctransformers](/bench_ctransformers/)     |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |
| [AutoGPTQ](/bench_autogptq/)               |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |
| [AutoAWQ](/bench_autoawq/)                 |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |
| [DeepSpeed](/bench_deepspeed/)             |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |
| [PyTorch Lightning](/bench_lightning/)     |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |
| [Optimum Nvidia](/bench_optimum_nvidia/)   |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |
| [Nvidia TensorRT-LLM](/bench_tensorrtllm/) |    âœ…    |    âœ…    |   âœ…    |   âœ…   |   âœ…   |   âœ…   |   âœ…   |     âœ…     |    âœ…     |


### Legend:
- âœ… Supported
- âŒ Not Supported
- âš ï¸ Supported but not implemented


### Some pointers to note:

1. For candle, Metal backend is supported but it gives terrible performance [even in small models like Phi2](https://github.com/huggingface/candle/issues/1568). For AMD ROCM there is no support as per this [issue](https://github.com/huggingface/candle/issues/346).
