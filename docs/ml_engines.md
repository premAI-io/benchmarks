# ğŸ”§ ML Engines

### Model Framework Support Matrix

| Engine                                     | Float32 | Float16 | Int8  | Int4  | CUDA  | ROCM  | Mac M1/M2 | Training |
| ------------------------------------------ | :-----: | :-----: | :---: | :---: | :---: | :---: | :-------: | :------: |
| [candle](/bench_candle/)                   |    âš ï¸    |    âœ…    |   âš ï¸   |   âš ï¸   |   âœ…   |   âŒ   |     ğŸš§     |    âŒ     |
| [llama.cpp](/bench_llamacpp/)              |    âŒ    |    âŒ    |   âœ…   |   âœ…   |   âœ…   |   ğŸš§   |     ğŸš§     |    âŒ     |
| [ctranslate](/bench_ctranslate/)           |    âœ…    |    âœ…    |   âœ…   |   âŒ   |   âœ…   |   âŒ   |     ğŸš§     |    âŒ     |
| [onnx](/bench_onnxruntime/)                |    âœ…    |    âœ…    |   âŒ   |   âŒ   |   âœ…   |   âš ï¸   |     âŒ     |    âŒ     |
| [transformers (pytorch)](/bench_pytorch/)  |    âœ…    |    âœ…    |   âœ…   |   âœ…   |   âœ…   |   ğŸš§   |     âœ…     |    âœ…     |
| [vllm](/bench_vllm/)                       |    âœ…    |    âœ…    |   âŒ   |   âœ…   |   âœ…   |   ğŸš§   |     âŒ     |    âŒ     |
| [exllamav2](/bench_exllamav2/)             |    âŒ    |    âŒ    |   âœ…   |   âœ…   |   âœ…   |   ğŸš§   |     âŒ     |    âŒ     |
| [ctransformers](/bench_ctransformers/)     |    âŒ    |    âŒ    |   âœ…   |   âœ…   |   âœ…   |   ğŸš§   |     ğŸš§     |    âŒ     |
| [AutoGPTQ](/bench_autogptq/)               |    âœ…    |    âœ…    |   âš ï¸   |   âš ï¸   |   âœ…   |   âŒ   |     âŒ     |    âŒ     |
| [AutoAWQ](/bench_autoawq/)                 |    âŒ    |    âŒ    |   âŒ   |   âœ…   |   âœ…   |   âŒ   |     âŒ     |    âŒ     |
| [DeepSpeed-MII](/bench_deepspeed/)         |    âŒ    |    âœ…    |   âŒ   |   âŒ   |   âœ…   |   âŒ   |     âŒ     |    âš ï¸     |
| [PyTorch Lightning](/bench_lightning/)     |    âœ…    |    âœ…    |   âœ…   |   âœ…   |   âœ…   |   âš ï¸   |     âš ï¸     |    âœ…     |
| [Optimum Nvidia](/bench_optimum_nvidia/)   |    âœ…    |    âœ…    |   âŒ   |   âŒ   |   âœ…   |   âŒ   |     âŒ     |    âŒ     |
| [Nvidia TensorRT-LLM](/bench_tensorrtllm/) |    âœ…    |    âœ…    |   âœ…   |   âœ…   |   âœ…   |   âŒ   |     âŒ     |    âŒ     |


### Legend:
- âœ… Supported
- âŒ Not Supported
- âš ï¸ There is a catch related to this
- ğŸš§ It is supported but not implemented in this current version


### Some pointers to note:
The names are by the name of engines. Except when the name is `Generic` then it means that the nuance applies to all the engines.


| Name              | Type | Description                                                                                                                                                                                                                            |
| ----------------- | ---- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| candle            | âš ï¸    | Metal backend is supported but it gives terrible performance even in small models like Phi2. For AMD ROCM there is no support as per this [issue](https://github.com/huggingface/candle/issues/346).                                   |
| candle            | ğŸš§    | Latest performance for Candle is not implemented. If you want to see the numbers, please check out [archive.md](/docs/archive.md) which contains the benchmark numbers for [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b). |
| ctranslate2       | âš ï¸    | ROCM is not supported; however, works are in progress to have this feature on CTranslate2. No support for Mac M1/M2.                                                                                                                   |
| onnxruntime       | âš ï¸    | ONNXRuntime in general supports ROCM, but specific to LLMs and ONNXRuntime with HuggingFace Optimum only supports CUDAExecution provider right now. For CPU, it is available but super slow.                                           |
| pytorch lightning | âš ï¸    | ROCM is supported but not tested for PyTorch Lightning. See this [issue](https://github.com/Lightning-AI/litgpt/issues/1220).                                                                                                          |
| pytorch lightning | âš ï¸    | Metal is supported in PyTorch Lightning, but for Llama 2 7B Chat or Mistral 7B, it is super slow.                                                                                                                                      |
| AutoGPTQ          | âš ï¸    | AutoGPTQ is a weight-only quantization algorithm. Activation still remains in either float32 or float16. We used a 4-bit weight quantized model for our benchmarks experiment.                                                         |
| Generic           | ğŸš§    | For all the engines which support metal, please check out [archive.md](/docs/archive.md) which contains the benchmark numbers for [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b).                                          |
| Deepspeed         | âš ï¸    | [DeepSpeed](https://github.com/microsoft/DeepSpeed) supports training; however, for inference, we have used [DeepSpeed MII](https://github.com/microsoft/DeepSpeed-MII).                                                               |
