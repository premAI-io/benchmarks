# ⚙️ Benchmarking ML Engines

## A100 80GB Inference Bench:

**Environment:**
- Model: Mistral 7B v0.1 Instruct
- CUDA Version: 12.1
- Command: `./benchmark.sh --repetitions 10 --max_tokens 512 --device cuda --model mistral --prompt 'Write an essay about the transformer model architecture'`

**Performance Metrics:** (unit: Tokens / second)

| Engine                                        | float32      | float16        | int8          | int4          |
|-----------------------------------------------|--------------|----------------|---------------|---------------|
| [HF Transformers (pytorch)](/bench_pytorch/)  | 39.27 ± 0.54 | 37.57 ± 0.36   | 5.03 ± 0.08   | 19.70 ± 0.30  |
| [AutoGPTQ](/bench_autogptq/)                  |       -      |       -        | 86.14 ± 1.40  | 87.22 ± 1.54  |

**Performance Metrics:** GPU Memory Consumption (unit: MB)

| Engine                                        | float32  | float16  | int8     | int4     |
|-----------------------------------------------|----------|----------|----------|----------|
| [HF Transformers (pytorch)](/bench_pytorch/)  | 31069.31 | 46030.39 | 23957.86 | 13935.58 |
| [AutoGPTQ](/bench_autogptq/)                  |     -    |     -    | 10255.07 | 6966.74  |


*(Data updated: `<LAST_UPDATE>`)
