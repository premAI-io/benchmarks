# ⚙️ Benchmarking ML Engines

## A100 80GB Inference Bench:

**Environment:**
- Model: Mistral 7B v0.1 Instruct
- CUDA Version: 12.1
- Command: `./benchmark.sh --repetitions 10 --max_tokens 512 --device cuda --model mistral --prompt 'Write an essay about the transformer model architecture'`

**Performance Metrics:** (unit: Tokens / second)

| Engine                                      | float32      | float16        | int8          | int4          |
|---------------------------------------------|--------------|----------------|---------------|---------------|
| [transformers (pytorch)](/bench_pytorch/)   | 39.27 ± 0.54 | 37.57 ± 0.36   | 5.03 ± 0.08   | 19.70 ± 0.30  |
| [AutoAWQ](/bench_autoawq/)                  |      -       |      -         |      -        | 63.12 ± 2.19  |
| [AutoGPTQ](/bench_autogptq/)                | 39.11 ± 0.42 | 42.94 ± 0.80   |               |               |
| [DeepSpeed](/bench_deepspeed/)              |              | 79.88 ± 0.32   |               |               |

**Performance Metrics:** GPU Memory Consumption (unit: MB)

| Engine                                      | float32  | float16  | int8     | int4     |
|---------------------------------------------|----------|----------|----------|----------|
| [transformers (pytorch)](/bench_pytorch/)   | 31069.31 | 46030.39 | 23957.86 | 13935.58 |
| [AutoGPTQ](/bench_autogptq/)                | 13400.80 | 6633.29  |          |          |
| [AutoAWQ](/bench_autoawq/)                  |      -   |      -   |      -   | 6572.47  |
| [DeepSpeed](/bench_deepspeed/)              |          | 80104.34 |          |          |


*(Data updated: `<LAST_UPDATE>`)
